{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Zillow’s Home Value Prediction\n",
    "#https://www.kaggle.com/c/zillow-prize-1/overview\n",
    "\n",
    "Zillow provides a “Zestimate”, which is an estimated property value.\n",
    "\n",
    "zillow's home value prediction:\n",
    "                       \n",
    "                       logerror = log(Zestimate) - log(SalePrice)\n",
    "\n",
    "\n",
    "#### 1. our task: predict the logerror of zillow with given house features\n",
    "\n",
    "#### 2. technical goals: \n",
    "\n",
    "- dealing with the missing data\n",
    "- defining an appropriate predicting model\n",
    "\n",
    "\n",
    "\n",
    "##### Reference: \n",
    "\n",
    "* https://towardsdatascience.com/fine-tuning-xgboost-in-python-like-a-boss-b4543ed8b1e\n",
    "\n",
    "* https://xgboost.readthedocs.io/en/latest/python/python_api.html\n",
    "\n",
    "* https://blog.cambridgespark.com/hyperparameter-tuning-in-xgboost-4ff9100a3b2f"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 129,
   "metadata": {},
   "outputs": [],
   "source": [
    "# import required packages\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import seaborn as sn\n",
    "import matplotlib.pyplot as plt\n",
    "import xgboost as xgb\n",
    "import random\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.model_selection import RandomizedSearchCV, GridSearchCV\n",
    "from sklearn.model_selection import StratifiedKFold\n",
    "from xgboost import XGBRegressor\n",
    "from sklearn.metrics import mean_squared_error\n",
    "from sklearn.metrics import mean_absolute_error\n",
    "from sklearn.preprocessing import LabelEncoder,OneHotEncoder\n",
    "%matplotlib inline"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 210,
   "metadata": {},
   "outputs": [],
   "source": [
    "# import dataset\n",
    "property_df = pd.read_csv('input/properties_2016.csv')\n",
    "train_df = pd.read_csv('input/train_2016_v2.csv')\n",
    "submission = pd.read_csv('input/sample_submission.csv')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 211,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Shape Train data (90275, 3)\n",
      "Shape Property (2985217, 58)\n"
     ]
    }
   ],
   "source": [
    "print('Shape Train data', train_df.shape)\n",
    "print('Shape Property', property_df.shape)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Features engineering\n",
    "\n",
    "- Target Variable : remove outliers\n",
    "- Missing data: remove columns with too many missing values\n",
    "- Imputing missing values and LabelEncoder"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 212,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(88465, 3)\n"
     ]
    }
   ],
   "source": [
    "# remove the outliers\n",
    "ulimit = np.percentile(train_df.logerror.values, 99)\n",
    "llimit = np.percentile(train_df.logerror.values, 1)\n",
    "train_df = train_df[train_df['logerror'] < ulimit]\n",
    "train_df = train_df[train_df['logerror'] > llimit]\n",
    "print(train_df.shape)\n",
    "\n",
    "missing_df = (property_df.isnull().sum()/property_df.isnull().count()).reset_index()\n",
    "missing_df.columns = ['column_name','counts']\n",
    "missing_df = missing_df[missing_df['counts']>0]\n",
    "missing_df = missing_df.sort_values(by='counts')\n",
    "\n",
    "# # barplot\n",
    "# ind = np.arange(missing_df.shape[0])\n",
    "# width = 0.9\n",
    "# fig, ax = plt.subplots(figsize=(12,18))\n",
    "# rects = ax.barh(ind, missing_df.counts, color='blue')\n",
    "# ax.set_yticks(ind)\n",
    "# ax.set_yticklabels(missing_df.column_name, rotation='horizontal')\n",
    "# ax.set_xlabel(\"Count of missing values\")\n",
    "# ax.set_title(\"Number of missing values in each column\")\n",
    "# plt.show()\n",
    "\n",
    "# remove columns\n",
    "drop_index = missing_df.column_name[missing_df.counts > 0.97]\n",
    "property_df = property_df.drop(drop_index,axis=1)\n",
    "\n",
    "\n",
    "# Imputing missing values and LabelEncoder\n",
    "for c in property_df:\n",
    "    property_df[c]=property_df[c].fillna(-1)\n",
    "    if property_df[c].dtype == 'object':\n",
    "        lbl = LabelEncoder()\n",
    "        lbl.fit(list(property_df[c].values))\n",
    "        property_df[c] = lbl.transform(list(property_df[c].values))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 216,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_train = train_df.merge(property_df, how='left', on='parcelid')\n",
    "train_x = df_train.drop(['parcelid', 'logerror','transactiondate'], axis=1)\n",
    "train_y = df_train['logerror'] "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 217,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "((59271, 40), (29194, 40))"
      ]
     },
     "execution_count": 217,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "X_train, X_test, y_train, y_test = train_test_split(train_x,train_y, test_size=0.33, random_state=42)\n",
    "X_train.shape,X_test.shape"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Mainly method: XGBOOST"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "# first round\n",
    "\n",
    "dtrain = xgb.DMatrix(X_train, label=y_train)\n",
    "dtest = xgb.DMatrix(X_test, label=y_test)\n",
    "\n",
    "# params\n",
    "# According to refrence one, I set eta: 0.01, num_boost_round: 999. I want to find the best boost round at this\n",
    "# learning rate\n",
    "# I use MAE(mean absolute error) to evaluate the quality of my predictions\n",
    "\n",
    "params = {\n",
    "    # Parameters that we are going to tune.\n",
    "    'max_depth':6,\n",
    "    'min_child_weight': 1,\n",
    "    'eta':0.01,\n",
    "    'subsample': 1,\n",
    "    'colsample_bytree': 1,\n",
    "    # Other parameters\n",
    "    'objective':'reg:linear',\n",
    "    'eval_metric':'mae'\n",
    "}\n",
    "\n",
    "num_boost_round = 999\n",
    "\n",
    "model = xgb.train(\n",
    "    params,\n",
    "    dtrain,\n",
    "    num_boost_round=num_boost_round,\n",
    "    evals=[(dtest, \"Test\")],\n",
    "    early_stopping_rounds=10\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 108,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Best MAE: 0.0532 with 737 rounds\n"
     ]
    }
   ],
   "source": [
    "print(\"Best MAE: {:.4f} with {} rounds\".format(\n",
    "                 model.best_score,\n",
    "                 model.best_iteration+1))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Here is the cv function from XGBoost. \n",
    "# It allows us to run cross-validation on our training dataset and returns a mean MAE score. \n",
    "\n",
    "cv_results = xgb.cv(\n",
    "    params,\n",
    "    dtrain,\n",
    "    num_boost_round=num_boost_round,\n",
    "    seed=42,\n",
    "    nfold=5,\n",
    "    metrics={'mae'},\n",
    "    early_stopping_rounds=10\n",
    ")\n",
    "\n",
    "cv_results\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 110,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.052557200000000005"
      ]
     },
     "execution_count": 110,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "cv_results['test-mae-mean'].min()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Tuning\n",
    "- Mainly method: RandomizedSearchCV "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 218,
   "metadata": {},
   "outputs": [],
   "source": [
    "# \n",
    "\n",
    "param_dist = {\n",
    "        'n_estimators':[737,738,739,740],\n",
    "        'max_depth':range(3,12),\n",
    "        'subsample':np.linspace(0.8,1,5),\n",
    "        'colsample_bytree':np.linspace(0.5,0.98,10),\n",
    "        'min_child_weight':range(1,9)\n",
    "        }\n",
    "\n",
    "\n",
    "\n",
    "xgb = XGBRegressor(learning_rate=0.01)\n",
    " \n",
    "\n",
    "random_search = RandomizedSearchCV(xgb, param_distributions=param_dist, \n",
    "                                   n_iter=5, scoring='neg_mean_absolute_error', \n",
    "                                   n_jobs=4, cv=5, \n",
    "                                   verbose=3, random_state=100)\n",
    "\n",
    "\n",
    "random_search.fit(X_train,y_train)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 125,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "({'subsample': 1.0,\n",
       "  'n_estimators': 740,\n",
       "  'min_child_weight': 6,\n",
       "  'max_depth': 6,\n",
       "  'colsample_bytree': 0.9266666666666666},\n",
       " XGBRegressor(base_score=0.5, booster='gbtree', colsample_bylevel=1,\n",
       "        colsample_bynode=1, colsample_bytree=0.9266666666666666, gamma=0,\n",
       "        importance_type='gain', learning_rate=0.01, max_delta_step=0,\n",
       "        max_depth=6, min_child_weight=6, missing=None, n_estimators=740,\n",
       "        n_jobs=1, nthread=None, objective='reg:linear', random_state=0,\n",
       "        reg_alpha=0, reg_lambda=1, scale_pos_weight=1, seed=None,\n",
       "        silent=None, subsample=1.0, verbosity=1),\n",
       " -0.05254329764833132)"
      ]
     },
     "execution_count": 125,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "random_search.best_params_, random_search.best_estimator_, random_search.best_score_"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 142,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Here I want to tune the learning_rate\n",
    "\n",
    "param_eta = {\n",
    "    'learning_rate':[.05, .01, .005]\n",
    "} \n",
    "\n",
    "xgb2 = XGBRegressor(max_depth=6, \n",
    "                   subsample=1, \n",
    "                   n_estimators=740,\n",
    "                   min_child_weight=6,\n",
    "                   colsample_bytree=0.927)\n",
    "\n",
    "gs = GridSearchCV(estimator = xgb2, param_grid = param_eta,cv=5,scoring='neg_mean_absolute_error')\n",
    "gs.fit(X_train, y_train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 141,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "({'learning_rate': 0.01},\n",
       " XGBRegressor(base_score=0.5, booster='gbtree', colsample_bylevel=1,\n",
       "        colsample_bynode=1, colsample_bytree=0.927, gamma=0,\n",
       "        importance_type='gain', learning_rate=0.01, max_delta_step=0,\n",
       "        max_depth=6, min_child_weight=6, missing=None, n_estimators=740,\n",
       "        n_jobs=1, nthread=None, objective='reg:linear', random_state=0,\n",
       "        reg_alpha=0, reg_lambda=1, scale_pos_weight=1, seed=None,\n",
       "        silent=None, subsample=1, verbosity=1),\n",
       " -0.05254329764833132)"
      ]
     },
     "execution_count": 141,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "gs.best_params_, gs.best_estimator_, gs.best_score_"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "#\n",
    "\n",
    "num_boost_round=740\n",
    "params = {\n",
    "    # Parameters that we are going to tune.\n",
    "    'max_depth':6,\n",
    "    'min_child_weight': 6,\n",
    "    'eta':0.01,\n",
    "    'subsample': 1,\n",
    "    'colsample_bytree': 0.927,\n",
    "    # Other parameters\n",
    "    'objective':'reg:linear',\n",
    "    'eval_metric':'mae'\n",
    "}\n",
    "\n",
    "\n",
    "best_model = xgb.train(\n",
    "    params,\n",
    "    dtrain,\n",
    "    num_boost_round=num_boost_round,\n",
    "    evals=[(dtest, \"Test\")]\n",
    ")\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 153,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.05313128537286346"
      ]
     },
     "execution_count": 153,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "mean_absolute_error(best_model.predict(dtest), y_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 207,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Predicting for: 201610 ... \n",
      "Predicting for: 201611 ... \n",
      "Predicting for: 201612 ... \n",
      "Predicting for: 201710 ... \n",
      "Predicting for: 201711 ... \n",
      "Predicting for: 201712 ... \n"
     ]
    }
   ],
   "source": [
    "#submit\n",
    "select_feature = list(X_train.columns)\n",
    "test_df = pd.merge(submission[['ParcelId']], property_df.rename(columns = {'parcelid': 'ParcelId'}), how = 'left', on = 'ParcelId')\n",
    "protest = xgb.DMatrix(test_df[select_feature])\n",
    "y_pred = best_model.predict(protest)\n",
    "\n",
    "\n",
    "submission = pd.DataFrame({\n",
    "    'ParcelId': test_df['ParcelId'],\n",
    "})\n",
    "\n",
    "test_dates = {\n",
    "    '201610': pd.Timestamp('2016-09-30'),\n",
    "    '201611': pd.Timestamp('2016-10-31'),\n",
    "    '201612': pd.Timestamp('2016-11-30'),\n",
    "    '201710': pd.Timestamp('2017-09-30'),\n",
    "    '201711': pd.Timestamp('2017-10-31'),\n",
    "    '201712': pd.Timestamp('2017-11-30')\n",
    "}\n",
    "for label, test_date in test_dates.items():\n",
    "    print(\"Predicting for: %s ... \" % (label))\n",
    "    submission[label] = y_pred\n",
    "\n",
    "submission.to_csv('my_submission2.csv', float_format='%.6f',index=False)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
